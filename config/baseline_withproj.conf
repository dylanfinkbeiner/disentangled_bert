// Config settings used for SuperGLUE BERT baseline experiments.

// This imports the defaults, which can be overridden below.
include "defaults.conf"
exp_name = "baseline-withproj"
run_name = "baseline-withproj-64k-power75"

cuda = 0
val_interval = 500
random_seed = 4321

// Data and preprocessing settings
max_seq_len = 256 // Mainly needed for MultiRC, to avoid over-truncating
                  // But not 512 as that is really hard to fit in memory.
tokenizer = bert-base-uncased
// Model settings
input_module = bert-base-uncased
bert_embeddings_mode = "top"
pool_type = "first"
pair_attn = 0 // shouldn't be needed but JIC
s2s = {
    attention = none
}
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg // following BERT paper
transfer_paradigm = finetune // finetune entire BERT model
pretrain_tasks = "cola,rte"
target_tasks = "cola,rte"

// Training settings
dropout = 0.1 // following BERT paper
optimizer = bert_adam
weighting_method = power_0.75  // following BAM paper
batch_size = 16
max_epochs = 4
lr = .00002
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000 // cannot be disabled, must be set very high to rely on max_epochs as stopping criterion

// Control-flow stuff
do_pretrain = 1
do_target_task_training = 0
do_full_eval = 1
write_preds = "val,test"
write_strict_glue_format = 1


// Parameters Specific to our Adversarial Experiments
special_task = True
k_syn = 64
k_sem = 64
// scale_adv_syn = -0.0001
// scale_adv_sem = -0.0001
